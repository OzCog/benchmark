
Measurement results
-------------------
Performed on `fanny`, a circa-2015 vintage AMD Opteron 6344 (Abu Dhabi)

14 January 2020
---------------
Got this with ordinary pages (not hugetlb):

Analyzed 681 genes in 137.09 seconds
Analyzed 681 genes in 141.01 seconds
Analyzed 681 genes in 141.91 seconds

Got this w/ hugetlb: (2MB pagesize)

Analyzed 681 genes in 106.90 seconds
Analyzed 681 genes in 105.89 seconds
Analyzed 681 genes in 107.60 seconds

This is using the AtomSpace of the day.

The above are the triangle-dataset, only. Repeat, with hugetlb, for the
combined dataset, triangles and pentagons:

Triangle relations for 681 genes in 131.95 seconds
Triangle relations for 681 genes in 141.52 seconds
Triangle relations for 681 genes in 141.10 seconds
Protein expression for 12 pathways in 101.21 seconds
Protein expression for 12 pathways in 105.71 seconds
Protein expression for 12 pathways in 107.90 seconds
Protein expression for 704 pathways in 4895.7 seconds

Wow!  Looks like loading up all that additional data slowed down
the triangle-search by 30%!  That's not ... good.

Memory usage, w/4K pages, from `ps aux`
This holds more-or-less steady for all of the benchmarks.
    VSZ   RSS
2013500 1706312

i.e. 2GB virtual size, 1.6GB working-set size.

25 November 2021
----------------
Hmm. Things slowed down... No hugetlb, and combined dataset:

Triangle relations for 681 genes in 211.70 seconds
Triangle relations for 681 genes in 204.04 seconds
Triangle relations for 681 genes in 204.16 seconds
Protein expression for 12 pathways in 136.37 seconds
Protein expression for 12 pathways in 136.18 seconds
Protein expression for 12 pathways in 138.09 seconds

This is with
gcc version 10.2.1 20210110 (Debian 10.2.1-6)
ldd (Debian GLIBC 2.31-13+deb11u2) 2.31
guile (GNU Guile) 3.0.7.6-22120

--------
Repeat with
HUGETLB_MORECORE=yes LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libhugetlbfs.so.0 guile -l bio-loop.scm
is faster, but not hitting the previous scores.

Triangle relations for 681 genes in 155.17 seconds
Triangle relations for 681 genes in 154.98 seconds
Triangle relations for 681 genes in 157.30 seconds
Protein expression for 12 pathways in 110.82 seconds
Protein expression for 12 pathways in 112.45 seconds
Protein expression for 12 pathways in 111.80 seconds

================================================================
OK. Here's the deal. The bio-loop3.scm benchmark measures parallelism.
It appears that parallelism results are highly dependent on the CPU
atomics implementation. The AtomSpace uses std::shared_ptr very heavily
for refernce counting, and this appears to use CPU atomics to maintain
the use counts. This results in very heavy contention for the atomics
implemented in the CPU. Older CPU's fare poorly on this benchmark!

And now for loop3 ... see if anything seems changed.
Triangle relations for 1 threads in 188.01 seconds
Rate 3.6222 genes/sec Rate per thread: 3.6222 genes/sec ... 3.4676 genes/sec
Threads: 2  speedup vs 1x = 1.4336 ... 1.4128
Threads: 3  speedup vs 1x = 1.7775 ... 1.7056
Threads: 4  speedup vs 1x = 2.0705 ... 2.0470
Threads: 6  speedup vs 1x = 2.5702 ... 2.5773
Threads: 8  speedup vs 1x = 2.7884 ... 2.8340
Threads: 12  speedup vs 1x = 2.7377 ... 2.9228
Threads: 16  speedup vs 1x = 2.8052 ... 2.9195

So above is two runs with the #2907 pull req.

----------
Below takes this, and tightens the locks slightly in TypeIndex.h
Triangle relations for 1 threads in 190.55 seconds
Rate 3.5739 genes/sec Rate per thread: 3.5739 genes/sec
Threads: 2  speedup vs 1x = 1.3757
Threads: 3  speedup vs 1x = 1.7257
Threads: 4  speedup vs 1x = 2.0463
Threads: 6  speedup vs 1x = 2.5495
Threads: 8  speedup vs 1x = 2.8462
Threads: 12  speedup vs 1x = 2.9247
Threads: 16  speedup vs 1x = 2.8963

Hmm. No noticable difference. So, whatever the bottlenecks are,
It doesn't seem to be in the atomspace typeindex locking.

---------
Travelling back 3 days earlier, before the sequence of pull reqs
reorganizing the locks, I get this:

Triangle relations for 1 threads in 184.39 seconds
Rate 3.6932 genes/sec Rate per thread: 3.6932 genes/sec
Threads: 2  speedup vs 1x = 1.3905
Threads: 3  speedup vs 1x = 1.7089
Threads: 4  speedup vs 1x = 2.0388
Threads: 6  speedup vs 1x = 2.6780
Threads: 8  speedup vs 1x = 2.9725
Threads: 12  speedup vs 1x = 3.3356
Threads: 16  speedup vs 1x = 3.3057

Well, crap. That's margially better than the new code. This may be
explainable by the fact that the new code is grabbing and releasing
locks more frequently.  If so, then the good news is that a lockless
design really should make things better. Hmmm...

---------
Try again, using facebook folly F14ValueSet in TypeIndex.

Triangle relations for 1 threads in 192.40 seconds
Rate 3.5395 genes/sec Rate per thread: 3.5395 genes/sec
Threads: 2  speedup vs 1x = 1.4063
Threads: 3  speedup vs 1x = 1.8001
Threads: 4  speedup vs 1x = 2.1576
Threads: 6  speedup vs 1x = 2.6433
Threads: 8  speedup vs 1x = 3.0247
Threads: 12  speedup vs 1x = 3.2299
Threads: 16  speedup vs 1x = 3.3724

Maybe a hair faster, but the speedup is mostly lost in the noise.

---------
Try again, with no lock at all for the type index. This won't pass unit
tests but it tells us about the parallelism bottleneck.

Triangle relations for 1 threads in 185.30 seconds
Rate 3.6752 genes/sec Rate per thread: 3.6752 genes/sec
Threads: 2  speedup vs 1x = 1.3970
Threads: 3  speedup vs 1x = 1.7669
Threads: 4  speedup vs 1x = 2.1386
Threads: 6  speedup vs 1x = 2.6839
Threads: 8  speedup vs 1x = 3.0602
Threads: 12  speedup vs 1x = 3.5631
Threads: 16  speedup vs 1x = 3.6043

OK, well, that's interesting. Seems that the TypeIndex lock has more
or less no effect at all on the parallelism.

---------
Try again, but with no lock on the incoming set.

Triangle relations for 1 threads in 185.99 seconds
Rate 3.6615 genes/sec Rate per thread: 3.6615 genes/sec
Threads: 2  speedup vs 1x = 1.4067
Threads: 3  speedup vs 1x = 1.7198
Threads: 4  speedup vs 1x = 2.0300
Threads: 6  speedup vs 1x = 2.5332
Threads: 8  speedup vs 1x = 2.7638
Threads: 12  speedup vs 1x = 2.9036
Threads: 16  speedup vs 1x = 2.9048

Holy cow! No effect at all on the parallelism. So ... where is the
bottleneck, then?

---------
Try again, with two differences:
1) data loaded from rocksdb
2) Both atom and type index running on f14:F14ValueNode

Triangle relations for 1 threads in 204.67 seconds  ... 200.54
Rate 3.3273 genes/sec Rate per thread: 3.3273 genes/sec  ... 3.3957 genes/sec
Threads: 2  speedup vs 1x = 1.5502 ... 1.5636
Threads: 3  speedup vs 1x = 1.9024 ... 1.8774
Threads: 4  speedup vs 1x = 2.2616 ... 2.1812
Threads: 6  speedup vs 1x = 2.9206 ... 2.8736
Threads: 8  speedup vs 1x = 3.3731 ... 3.3156
Threads: 12  speedup vs 1x = 3.8862 ... 3.7239
Threads: 16  speedup vs 1x = 3.7935 ... 3.7622

So .. speedups are quite significant... underwhelming, but significant.

---------
Try again, with three differences:
1) data loaded from rocksdb
2) Type index running on f14:F14ValueNode
3) Incoming index running on std::unordered_set


Triangle relations for 1 threads in 200.97 seconds
Rate 3.3885 genes/sec Rate per thread: 3.3885 genes/sec
Threads: 2  speedup vs 1x = 1.5644
Threads: 3  speedup vs 1x = 1.8654
Threads: 4  speedup vs 1x = 2.2194
Threads: 6  speedup vs 1x = 2.9437
Threads: 8  speedup vs 1x = 3.3309
Threads: 12  speedup vs 1x = 3.7352
Threads: 16  speedup vs 1x = 3.7741

conclude: incoming index on unordered_set is same as f14
We cannot use the F14 on the type index due to some bug.

---------
Try again, with three differences:
1) data loaded from rocksdb
2) f14 disabled entirely.

Triangle relations for 1 threads in 219.24 seconds ... 216.71 seconds
Rate 3.1062 genes/sec Rate per thread: 3.1062 genes/sec ... 3.1424 genes/sec
Threads: 2  speedup vs 1x = 1.5547 ... 1.5845
Threads: 3  speedup vs 1x = 1.8071 ... 1.8550
Threads: 4  speedup vs 1x = 2.1617 ... 2.2281
Threads: 6  speedup vs 1x = 2.8369 ... 2.9055
Threads: 8  speedup vs 1x = 3.3070 ... 3.4421
Threads: 12  speedup vs 1x = 3.8141 ... 3.8943
Threads: 16  speedup vs 1x = 3.9205 ... 3.9716

I'm confused. The speedups are bettter than before. But the baseline
is a lot lower, so ... ??

---------
Try again, with three differences:
1) data loaded from rocksdb
2) f14 only in incoming, where it's allowable
   (cannot use it on type index due to bug.)

Triangle relations for 1 threads in 208.79 seconds
Rate 3.2617 genes/sec Rate per thread: 3.2617 genes/sec
Threads: 2  speedup vs 1x = 1.5522
Threads: 3  speedup vs 1x = 1.9315
Threads: 4  speedup vs 1x = 2.2490
Threads: 6  speedup vs 1x = 3.0139
Threads: 8  speedup vs 1x = 3.5071
Threads: 12  speedup vs 1x = 3.9536
Threads: 16  speedup vs 1x = 3.9083

Conclude, comparing to earlier: guile gc interferes with
parallelizability; loading from rocksdb shrinks the hit
on guile.

---------
Lets check above conclusion: try again
1) data loaded from rocksdb
2) locks removed from TypeIndex.

Triangle relations for 1 threads in 211.25 seconds
Rate 3.2237 genes/sec Rate per thread: 3.2237 genes/sec
Threads: 2  speedup vs 1x = 1.4585
Threads: 3  speedup vs 1x = 1.6676
Threads: 4  speedup vs 1x = 2.0075
Threads: 6  speedup vs 1x = 2.5982
Threads: 8  speedup vs 1x = 3.0789
Threads: 12  speedup vs 1x = 3.3719
Threads: 16  speedup vs 1x = 3.3396

I'm confused: how is it that the code without locks is slower
than the code with locks? Is this a cache-line issue?

mutrace is not providing adequate insight.

---------
Check standard AtomSpace on aimdee. Wow!

Triangle relations for 1 threads in 63.278 seconds
Rate 10.762 genes/sec Rate per thread: 10.762 genes/sec
Threads: 2  speedup vs 1x = 1.8702
Threads: 3  speedup vs 1x = 2.6588
Threads: 4  speedup vs 1x = 3.3858
Threads: 6  speedup vs 1x = 3.9591
Threads: 8  speedup vs 1x = 4.4759
Threads: 12  speedup vs 1x = 4.4349
Threads: 16  speedup vs 1x = 4.4185

This is an 4-core AMD Ryzen 5 3400G with 8 hypercores, I guess

Same benchmark, under load from other computes is even more revealing:

Triangle relations for 1 threads in 116.41 seconds
Rate 5.8500 genes/sec Rate per thread: 5.8500 genes/sec
Threads: 2  speedup vs 1x = 1.9469
Threads: 3  speedup vs 1x = 2.9243
Threads: 4  speedup vs 1x = 3.6402
Threads: 6  speedup vs 1x = 5.7024
Threads: 8  speedup vs 1x = 7.2898
Threads: 12  speedup vs 1x = 7.3245
Threads: 16  speedup vs 1x = 7.2369

So, under load this scales nicely all the way to 8 threads! Wow!

---------
But we cannot always be this lucky.
On a different machine:
AMD Ryzen 9 3900X 12-Core Processor
24 hyperthreads the gain is more modest:

Under load:
Triangle relations for 1 threads in 92.526 seconds
Rate 7.3601 genes/sec Rate per thread: 7.3601 genes/sec
Threads: 2  speedup vs 1x = 1.5080
Threads: 3  speedup vs 1x = 1.9066
Threads: 4  speedup vs 1x = 2.2924
Threads: 6  speedup vs 1x = 3.0969
Threads: 8  speedup vs 1x = 3.7011
Threads: 12  speedup vs 1x = 4.6900
Threads: 16  speedup vs 1x = 5.6154
Threads: 24  speedup vs 1x = 6.9823

so that is underwhelming; not as good as above.
Even more so without a load:

Triangle relations for 1 threads in 47.933 seconds
Rate 14.207 genes/sec Rate per thread: 14.207 genes/sec
Threads: 2  speedup vs 1x = 1.2938
Threads: 3  speedup vs 1x = 1.5265
Threads: 4  speedup vs 1x = 1.7072
Threads: 6  speedup vs 1x = 2.1583
Threads: 8  speedup vs 1x = 2.4231
Threads: 12  speedup vs 1x = 2.8141
Threads: 16  speedup vs 1x = 3.1561
Threads: 24  speedup vs 1x = 3.7539

so single-thread serial is much better; but even with two threads,
the speedup is terrible.
